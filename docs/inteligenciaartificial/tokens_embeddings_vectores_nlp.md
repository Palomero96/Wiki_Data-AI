# Tokens, Embeddings, Vectores y Aprendizaje Transferido en NLP

Este documento explica conceptos fundamentales para entender c√≥mo los modelos de lenguaje procesan texto, representan informaci√≥n y se adaptan a nuevas tareas.

---

## 1. Tokens

* **Definici√≥n**: Unidad m√≠nima de texto que un modelo procesa. Puede ser palabra, subpalabra o car√°cter.
* **Tipos**:

  * Palabras completas: "gato", "perro"
  * Subpalabras (BPE, WordPiece): "g", "##ato", "per", "##ro"
  * Caracteres: "g", "a", "t", "o"
* **Ejemplo**:

```
Texto: "Inteligencia artificial"
Tokens (WordPiece): ["Int", "##eligencia", "artificial"]
```

* **Importancia**: La tokenizaci√≥n define c√≥mo se construyen los embeddings y la entrada al modelo.

---

## 2. Embeddings

* **Definici√≥n**: Representaci√≥n vectorial densa de tokens o entidades que captura significado sem√°ntico.
* **Propiedades**:

  * Vectores cercanos ‚Üí palabras similares
  * Permiten operaciones sem√°nticas:

$$
v_{rey} - v_{hombre} + v_{mujer} \approx v_{reina}
$$

* **Dimensi√≥n t√≠pica**: 50-1024 seg√∫n modelo.
* **Diagrama conceptual:**

```
Tokens --> Embedding Layer --> Vectores densos
```

---

## 3. Vectores y B√∫squeda Vectorial

### 3.1 Vectores vs Embeddings

* **Vector**: Lista de n√∫meros en un espacio n-dimensional, puede representar cualquier dato.
* **Embedding**: Vector especial que captura relaciones sem√°nticas o contextuales.
* **Resumen visual:**

```
Todos los embeddings son vectores
Pero no todos los vectores son embeddings
```

### 3.2 B√∫squeda de Vectores

* **Objetivo**: Encontrar elementos similares en un espacio vectorial.
* **Usos**: Recuperaci√≥n de informaci√≥n, recomendaci√≥n, b√∫squeda sem√°ntica.
* **M√©tricas de similitud**:

  * Cosine similarity:

$$
\text{sim}(u,v) = \frac{u \cdot v}{\|u\| \|v\|}
$$

* Distancia euclidiana:

$$
d(u,v) = \sqrt{\sum_i (u_i - v_i)^2}
$$

### 3.3 Ejemplo conceptual de b√∫squeda

```
Embeddings:
v_gato = [0.21, -0.11, 0.56]
v_perro = [0.19, -0.09, 0.53]
v_le√≥n = [0.40, 0.12, 0.80]

Consulta: "gato"
Similitudes:
cos(v_gato, v_perro) = 0.98
cos(v_gato, v_le√≥n) = 0.75

Resultado: "perro" (m√°s similar sem√°nticamente)
```

* **Diagrama conceptual de b√∫squeda:**

```
Consulta (embedding) --> Comparar con embeddings en la base --> Ranking por similitud --> Resultado m√°s cercano
```

### 3.4 Bases de Datos Vectoriales e √çndices

* **Definici√≥n**: Sistemas especializados para almacenar y buscar embeddings eficientemente.

* **Objetivo**: Permitir b√∫squeda r√°pida y escalable de vectores en alta dimensi√≥n.

* **Ejemplos de bases vectoriales**:

  * FAISS (Facebook AI Similarity Search)
  * Milvus
  * Pinecone

* **√çndices comunes y detalles**:

  * **HNSW (Hierarchical Navigable Small World)**: Estructura de grafo que permite b√∫squeda aproximada de vecinos m√°s cercanos con alta eficiencia y precisi√≥n.
  * **IVF (Inverted File Index)**: Divide el espacio vectorial en clusters y busca primero en el cluster m√°s relevante, acelerando la recuperaci√≥n.
  * **Annoy (Approximate Nearest Neighbors)**: √Årboles aleatorios que permiten b√∫squedas r√°pidas con aproximaciones en espacios de alta dimensi√≥n.
  * **PQ (Product Quantization)**: Reduce la memoria necesaria para almacenar vectores grandes, permitiendo b√∫squedas m√°s r√°pidas con compresi√≥n.

* **Diagrama conceptual:**

```
Embeddings --> Indexaci√≥n (HNSW / IVF / Annoy / PQ) --> B√∫squeda Aproximada --> Resultados m√°s similares
```

---

## 4. Fine-Tuning

* **Definici√≥n**: Ajustar un modelo preentrenado a una tarea espec√≠fica.
* **Proceso**:

  1. Tomar un modelo preentrenado (BERT, GPT)
  2. Agregar capa(s) espec√≠fica(s) para la tarea
  3. Entrenar con datos de la tarea
* **Ejemplo conceptual:**

```
BERT preentrenado --> Clasificador de sentimiento --> Dataset de rese√±as
```

* **Ventaja**: Reduce necesidad de grandes cantidades de datos y tiempo de entrenamiento.

---

## 5. Transfer Learning

* **Definici√≥n**: Reutilizar conocimiento aprendido en una tarea para otra tarea relacionada.
* **Tipos en NLP**:

  * Feature-based: usar embeddings como features
  * Fine-tuning: ajustar todo el modelo a la nueva tarea
* **Ejemplo conceptual:**

```
Modelo preentrenado en Wikipedia --> Clasificaci√≥n de noticias --> Clasificaci√≥n de tweets
```

* **Beneficio**: Permite aprovechar modelos grandes preentrenados sin entrenar desde cero.

---

## 6. Flujo Conceptual Completo


```
Texto crudo 
     ‚Üì
Tokenizaci√≥n 
     ‚Üì
Embeddings 
     ‚Üì
B√∫squeda / Comparaci√≥n de vectores (DB Vectorial e √≠ndices)
     ‚Üì
Modelo preentrenado 
     ‚Üì
Fine-tuning / Transfer Learning 
     ‚Üì
Tarea espec√≠fica
```

---

## üìå Conclusi√≥n

* **Tokens**: Base del procesamiento de texto.
* **Embeddings**: Vectores densos que capturan significado sem√°ntico.
* **B√∫squeda vectorial y bases de datos**: Permiten encontrar similitudes sem√°nticas de forma r√°pida y escalable; los √≠ndices optimizan la b√∫squeda aproximada.
* **Fine-tuning**: Adaptaci√≥n espec√≠fica de modelos grandes.
* **Transfer learning**: Reutilizaci√≥n de conocimiento preentrenado.

> La combinaci√≥n de estos conceptos es la base para construir sistemas NLP modernos robustos y eficientes.
