# Aprendizaje Supervisado

El **aprendizaje supervisado** consiste en aprender una funci√≥n a partir de un conjunto de datos etiquetados.
Formalmente, dado un conjunto de entrenamiento:

$$
D = \{(x_1, y_1), (x_2, y_2), \dots, (x_n, y_n)\}
$$

donde $x_i \in \mathbb{R}^d$ son los atributos y $y_i$ la etiqueta asociada, el objetivo es encontrar una funci√≥n $f: \mathbb{R}^d \to Y$ que aproxime la relaci√≥n entre entradas y salidas.

---

## Algoritmos Principales

### 1. √Årboles de Decisi√≥n

Los **√°rboles de decisi√≥n** dividen el espacio de datos en regiones homog√©neas mediante reglas jer√°rquicas.

**Medidas para seleccionar atributos:**

* **Entrop√≠a**

$$
H(S) = - \sum_{i=1}^k p_i \log_2(p_i)
$$

* **Ganancia de informaci√≥n**

$$
IG(S, A) = H(S) - \sum_{v \in Values(A)} \frac{|S_v|}{|S|} H(S_v)
$$

* **√çndice Gini**

$$
Gini(S) = 1 - \sum_{i=1}^k p_i^2
$$

**Algoritmos m√°s usados:**

* **ID3**: usa ganancia de informaci√≥n.
* **C4.5**: mejora ID3 con atributos continuos y poda.
* **CART**: usa √≠ndice Gini, puede generar √°rboles de regresi√≥n.

---

### 2. Reglas de Clasificaci√≥n

Se representan como **IF condiciones ‚Üí THEN clase**.

**Medidas de calidad de reglas:**

* **Soporte**:

$$
Support(A \to B) = \frac{|A \cap B|}{N}
$$

* **Confianza**:

$$
Confidence(A \to B) = \frac{|A \cap B|}{|A|}
$$

* **Lift**:

$$
Lift(A \to B) = \frac{Confidence(A \to B)}{P(B)}
$$

**Algoritmos m√°s comunes:**

* **ZeroR, OneR, RIPPER** para clasificaci√≥n.
* **Apriori** para asociaci√≥n de √≠tems.

---

### 3. k-Nearest Neighbors (k-NN)

Clasifica una muestra seg√∫n las etiquetas de sus **k vecinos m√°s cercanos** en el espacio de caracter√≠sticas.

**Distancias m√°s usadas:**

* **Eucl√≠dea**:

$$
d(x, y) = \sqrt{\sum_{i=1}^n (x_i - y_i)^2}
$$

* **Manhattan**:

$$
d(x, y) = \sum_{i=1}^n |x_i - y_i|
$$

La clase predicha es:

$$
\hat{y} = \arg\max_{c \in C} \sum_{i=1}^k \mathbf{1}(y_i = c)
$$

---

### 4. Support Vector Machines (SVM)

Buscan un **hiperplano √≥ptimo** que maximiza el margen entre clases:

$$
\max_{\mathbf{w}, b} \ \frac{2}{\|\mathbf{w}\|}
$$

sujeto a:

$$
y_i(\mathbf{w} \cdot \mathbf{x}_i + b) \geq 1, \quad \forall i
$$

**Funci√≥n de decisi√≥n:**

$$
f(x) = \text{sign}(\mathbf{w} \cdot \mathbf{x} + b)
$$

Con kernels $K(x, x')$ se pueden separar datos no lineales.

---

### 5. Regresi√≥n Lineal y Log√≠stica

**Regresi√≥n lineal**:

$$
y = \beta_0 + \beta_1 x_1 + \dots + \beta_p x_p + \epsilon
$$

Se estima minimizando el error cuadr√°tico medio:

$$
MSE = \frac{1}{n}\sum_{i=1}^n (y_i - \hat{y}_i)^2
$$

**Regresi√≥n log√≠stica**:
Se basa en la funci√≥n sigmoide:

$$
P(y=1|x) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 x_1 + \dots + \beta_p x_p)}}
$$

La decisi√≥n se toma con un umbral, t√≠picamente 0.5.

---

### 6. Redes Neuronales Artificiales

Inspiradas en el cerebro, est√°n formadas por **neuronas artificiales**.

**Modelo de neurona:**

$$
z = \sum_{i=1}^n w_i x_i + b
$$

$$
a = \phi(z)
$$

donde $\phi$ es una **funci√≥n de activaci√≥n**: sigmoide, ReLU, tanh.

**Perceptr√≥n:** algoritmo b√°sico de clasificaci√≥n binaria.

**Multilayer Perceptron (MLP):** varias capas ocultas entrenadas con **retropropagaci√≥n**:

$$
w_{ij} \leftarrow w_{ij} - \eta \frac{\partial L}{\partial w_{ij}}
$$

donde $\eta$ es la tasa de aprendizaje y $L$ la funci√≥n de p√©rdida (ej. entrop√≠a cruzada).

---

## üìå Conclusi√≥n

El aprendizaje supervisado abarca algoritmos desde modelos simples e interpretables (regresi√≥n, √°rboles, reglas) hasta modelos complejos y potentes (SVM, redes neuronales).
Cada uno tiene ventajas y limitaciones:

* **√Årboles y reglas**: interpretabilidad.
* **k-NN y SVM**: precisi√≥n en clasificaci√≥n.
* **Regresi√≥n**: simplicidad y base estad√≠stica.
* **Redes neuronales**: flexibilidad para problemas complejos.
