# Procesamiento de Lenguaje Natural (NLP) y Modelos de Lenguaje

El **Procesamiento de Lenguaje Natural (NLP)** permite que las m谩quinas comprendan, generen y analicen texto. Los **modelos de lenguaje** aprenden patrones para tareas como traducci贸n, resumen, clasificaci贸n o generaci贸n de texto.

---

## 1. Algoritmos Cl谩sicos de NLP

### 1.1 Bag of Words (BoW)
* Representa texto como vectores de frecuencia de palabras.
* Ejemplo:
Texto 1: "El gato duerme"
Texto 2: "El perro corre"

Vocabulario: ["El", "gato", "duerme", "perro", "corre"]
BoW Texto1: [1, 1, 1, 0, 0]
BoW Texto2: [1, 0, 0, 1, 1]


### 1.2 TF-IDF
* Pondera palabras seg煤n importancia relativa.
$$
TF\text{-}IDF(w,d) = TF(w,d) \cdot \log \frac{N}{DF(w)}
$$

### 1.3 Word Embeddings
* Representaciones densas que capturan significado sem谩ntico.
* **Word2Vec**: CBOW y Skip-Gram.
* Ejemplo conceptual:
$$
v_{king} - v_{man} + v_{woman} \approx v_{queen}
$$

---

## 2. Short Language Models (SLMs)

### 2.1 N-gram Models
* Predice siguiente palabra basada en las \(n-1\) anteriores:
$$
P(w_1, ..., w_T) \approx \prod_{t=1}^{T} P(w_t | w_{t-(n-1)}, ..., w_{t-1})
$$

### 2.2 LSTM (Long Short-Term Memory)
* Captura dependencias largas en secuencias.
* F贸rmula de actualizaci贸n de celda:
$$
c_t = f_t \odot c_{t-1} + i_t \odot \tilde{c}_t
$$

*Diagrama conceptual:*
x_t ---> [Input Gate] --->
[Forget Gate] ---> c_t ---> [Output Gate] ---> h_t


---

## 3. Large Language Models (LLMs)

### 3.1 Transformer
* Procesa secuencias completas con **atenci贸n**.
* F贸rmula de Attention:
$$
Attention(Q,K,V) = softmax\left(\frac{QK^T}{\sqrt{d_k}}\right) V
$$
*Diagrama conceptual:*
Input Embeddings --> [Multi-Head Attention] --> [Feed-Forward] --> Output
| ^
+-------- Positional Encoding ------------+


### 3.2 GPT (Generative Pretrained Transformer)
* Genera texto de manera autoregresiva.
* Preentrenamiento: predice siguiente palabra.
* Ejemplo te贸rico:
Input: "La capital de Francia es"
Predicci贸n: "Par铆s"

### 3.3 BERT (Bidirectional Encoder Representations)
* Captura contexto bidireccional.
* Preentrenamiento: Masked Language Modeling y Next Sentence Prediction.
* Ejemplo:
Input: "El gato est谩 [MASK] en la cama"
Predicci贸n: "durmiendo"

---

## 4. T茅cnicas Complementarias

* **Sentence Embeddings**: Representaci贸n vectorial de oraciones (Sentence-BERT).
* **Fine-Tuning / Prompting**: Ajuste de modelos grandes a tareas espec铆ficas.
* **Transfer Learning**: Reutilizar conocimiento de modelos preentrenados.

*Diagrama conceptual simplificado de flujo NLP a LLM:*
Texto Crudo --> Tokenizaci贸n --> Embeddings --> Modelo (SLM o LLM) --> Tarea (Clasificaci贸n, Generaci贸n, Resumen)

---

##  Conclusi贸n

* **SLMs**: N-grams, RNN, LSTM, 煤tiles para corpus peque帽os y tareas espec铆ficas.  
* **LLMs**: Transformers, GPT, BERT, capturan contexto amplio y generan lenguaje coherente.  
* **Word Embeddings**: Base para casi todos los modelos modernos.  
* **Selecci贸n**: Depende de tama帽o del corpus, complejidad de la tarea y recursos computacionales.