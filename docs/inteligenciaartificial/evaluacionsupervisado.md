# Evaluaci√≥n de Algoritmos de Aprendizaje Supervisado

La **evaluaci√≥n de modelos supervisados** es fundamental para medir su capacidad de generalizar sobre nuevos datos y evitar problemas como el sobreajuste (*overfitting*). A continuaci√≥n, se presentan las principales t√©cnicas y m√©tricas utilizadas.

---

## 1. Validaci√≥n de Modelos

### 1.1 Conjunto de Entrenamiento y Prueba

* Se divide el dataset en **train set** (para entrenar) y **test set** (para evaluar).
* Ejemplo t√≠pico: 70% entrenamiento, 30% prueba.
* **Problema**: la evaluaci√≥n depende mucho de c√≥mo se haga la partici√≥n.

### 1.2 Validaci√≥n Cruzada (Cross-Validation)

* Divide los datos en *k* particiones.
* Se entrena en *k-1* y se eval√∫a en la partici√≥n restante.
* El proceso se repite *k* veces y se promedia el resultado.
* **Ventaja**: m√°s robusto que un simple train/test split.
* **Desventaja**: mayor coste computacional.

**Caso especial:** *Stratified k-Fold* mantiene la proporci√≥n de clases en cada partici√≥n (√∫til en clasificaci√≥n desbalanceada).

### 1.3 Conjunto de Validaci√≥n

* Adem√°s de train/test, se reserva un conjunto adicional para ajustar hiperpar√°metros.
* Evita que el test set influya en el entrenamiento.

---

## 2. M√©tricas para Clasificaci√≥n

### 2.1 Matriz de Confusi√≥n

Permite ver **d√≥nde falla el modelo**. Ejemplo en un problema binario:

|                   | Predicci√≥n Positiva       | Predicci√≥n Negativa       |
| ----------------- | ------------------------- | ------------------------- |
| **Real Positivo** | Verdaderos Positivos (TP) | Falsos Negativos (FN)     |
| **Real Negativo** | Falsos Positivos (FP)     | Verdaderos Negativos (TN) |

### 2.2 Exactitud (Accuracy)

Mide el porcentaje de predicciones correctas.

$$
Accuracy = \frac{TP + TN}{TP + TN + FP + FN}
$$

* **Ventaja**: intuitiva.
* **Problema**: enga√±osa en clases desbalanceadas (ej. 95% de exactitud si el 95% de ejemplos son de una sola clase).

### 2.3 Precisi√≥n (Precision)

De todas las predicciones positivas, cu√°ntas son correctas.

$$
Precision = \frac{TP}{TP + FP}
$$

* Alta precisi√≥n = pocos falsos positivos.
* Ejemplo: √∫til en diagn√≥stico m√©dico cuando un falso positivo es muy costoso.

### 2.4 Recall o Sensibilidad (True Positive Rate)

De todos los positivos reales, cu√°ntos detect√≥ el modelo.

$$
Recall = \frac{TP}{TP + FN}
$$

* Alta sensibilidad = pocos falsos negativos.
* Ejemplo: √∫til en detecci√≥n de fraudes, donde es peor dejar escapar un fraude que marcar de m√°s.

### 2.5 Especificidad (True Negative Rate)

De todos los negativos reales, cu√°ntos detect√≥ bien.

$$
Specificity = \frac{TN}{TN + FP}
$$

* Complementa al recall: uno mide bien positivos, el otro negativos.

### 2.6 Medida F1

Promedio arm√≥nico de precisi√≥n y recall.

$$
F1 = 2 \cdot \frac{Precision \cdot Recall}{Precision + Recall}
$$

* √ötil cuando hay desbalance de clases.
* Penaliza cuando precisi√≥n y recall est√°n descompensados.

### 2.7 Curva ROC y AUC

* ROC: representa \$TPR\$ frente a \$FPR\$.
* AUC (√Årea Bajo la Curva): mide la capacidad global del clasificador para distinguir entre clases.
* Un modelo aleatorio tiene AUC = 0.5; un modelo perfecto, AUC = 1.

---

## 3. M√©tricas para Regresi√≥n

### 3.1 Error Cuadr√°tico Medio (MSE)

$$
MSE = \frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2
$$

* Penaliza m√°s los errores grandes.
* Muy usado en problemas de predicci√≥n continua.

### 3.2 Ra√≠z del Error Cuadr√°tico Medio (RMSE)

$$
RMSE = \sqrt{MSE}
$$

* Interpretable en las mismas unidades que la variable objetivo.
* Ejemplo: si predices precios de casas en ‚Ç¨, RMSE = 5000 significa un error promedio de 5000 ‚Ç¨.

### 3.3 Error Absoluto Medio (MAE)

$$
MAE = \frac{1}{n} \sum_{i=1}^n |y_i - \hat{y}_i|
$$

* Menos sensible a valores at√≠picos que MSE.
* Representa el error medio absoluto en las unidades originales.

### 3.4 Coeficiente de Determinaci√≥n (\$R^2\$)

$$
R^2 = 1 - \frac{\sum (y_i - \hat{y}_i)^2}{\sum (y_i - \bar{y})^2}
$$

* \$R^2 = 1\$: predicci√≥n perfecta.
* \$R^2 = 0\$: el modelo no mejora la media.
* Puede ser negativo si el modelo es peor que usar la media.

---

## 4. Problemas Comunes

### 4.1 Sobreajuste (Overfitting)

* El modelo aprende demasiado bien el entrenamiento pero falla en datos nuevos.
* **S√≠ntoma**: alta exactitud en train, baja en test.
* **Soluciones**: regularizaci√≥n, poda de √°rboles, early stopping, m√°s datos.

### 4.2 Subajuste (Underfitting)

* El modelo es demasiado simple y no captura la complejidad de los datos.
* **S√≠ntoma**: bajo rendimiento tanto en train como en test.
* **Soluciones**: usar modelos m√°s complejos, a√±adir caracter√≠sticas relevantes.

### 4.3 Desbalance de Clases

* Ocurre cuando una clase es mucho m√°s frecuente.
* **Problema**: el modelo puede predecir siempre la clase mayoritaria y obtener alta accuracy.
* **Soluciones**: *oversampling*, *undersampling*, m√©tricas alternativas (F1, AUC).

---

## üìå Conclusi√≥n

La evaluaci√≥n de algoritmos supervisados no se limita a una sola m√©trica. La elecci√≥n depende del tipo de problema:

* **Clasificaci√≥n balanceada**: Accuracy.
* **Clasificaci√≥n desbalanceada**: Precision, Recall, F1, AUC.
* **Regresi√≥n**: MSE, RMSE, MAE, \$R^2\$.

Una buena pr√°ctica es usar **varias m√©tricas y validaci√≥n cruzada** para obtener una visi√≥n completa del rendimiento del modelo.
